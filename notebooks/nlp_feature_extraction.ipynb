{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyLDAvis.gensim\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.io.arff import loadarff\n",
    "\n",
    "from src.data_preprocessing.data_preprocessor import (\n",
    "    load_data,\n",
    "    change_to_numeric,\n",
    "    feature_engineering,\n",
    "    transform_label,\n",
    "    convert_float_to_int,\n",
    "    drop_cols,\n",
    "    filter_na,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_DATA_PATH = Path(\"../../data/dataset.csv\")\n",
    "DATA_PATH = Path(\"../../data/file65ef3a759daf.arff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(data_path: Path, cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Load data and perform preprocessing steps.\n",
    "\n",
    "    Parameters\n",
    "    -------\n",
    "    path_ : Path\n",
    "            Path of the data.\n",
    "    cols : List\n",
    "            List of columns to drop.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : pd.DataFrame\n",
    "            Processed data.\n",
    "    \"\"\"\n",
    "    # load the data\n",
    "    data, _ = load_data(data_path)\n",
    "\n",
    "    # convert yes/no to 0/1 and year to int\n",
    "    data = change_to_numeric(data)\n",
    "    data = convert_float_to_int(data)\n",
    "\n",
    "    # perform feature engineering on state columns\n",
    "    data = feature_engineering(data)\n",
    "\n",
    "    # drop unwished cols\n",
    "    data = drop_cols(data, cols)\n",
    "\n",
    "    # transform label to 0/1 for citation\n",
    "    data = transform_label(data)\n",
    "\n",
    "    # filter na\n",
    "    data = filter_na(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df: pd.DataFrame, column_name: str = \"Description\") -> pd.DataFrame:\n",
    "    \"\"\"reformats text so that LDA can be applied in the next step\n",
    "\n",
    "    Parameters\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "            Data to transform.\n",
    "    column_name : (str, optional)\n",
    "            Name of column to be transformed.\n",
    "            Defaults to \"Description\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "            Transformed data.\n",
    "    \"\"\"\n",
    "    # making text lower case\n",
    "    lowercase_text = df[column_name].apply(lambda x: x.lower())\n",
    "\n",
    "    # tokenize text\n",
    "    tokenized_text = lowercase_text.apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "    # removing stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    clean_text = tokenized_text.apply(\n",
    "        lambda tokens: [\n",
    "            word for word in tokens if word not in stop_words and word not in string.punctuation\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # lemmatize words\n",
    "    lemmatized_text = clean_text.apply(\n",
    "        lambda tokens: [token.lemma_ for token in nlp(\" \".join(tokens))]\n",
    "    )\n",
    "\n",
    "    df[\"description_clean\"] = lemmatized_text.apply(lambda lem_tokens: \" \".join(lem_tokens))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_topics(\n",
    "    df: pd.DataFrame,\n",
    "    column_name: str = \"description_clean\",\n",
    "    n_topics: int = 10,\n",
    "    random_state: int = 42,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Applies LDA to a text column of DF and adds LDA topic distributions as new features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Data to transform.\n",
    "    column_name : str, optional\n",
    "        Name of the column to be transformed. Defaults to \"description_clean\".\n",
    "    num_topics : int, optional\n",
    "        Number of topics for LDA. Defaults to 10.\n",
    "    random_stat : int\n",
    "            Random state of split for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        Transformed data with added LDA topic features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Preprocess text\n",
    "    df = preprocess_text(df, column_name)\n",
    "\n",
    "    # Tokenized text is already available from preprocessing but needs to be list of list\n",
    "    tokenized_text = df[column_name].apply(lambda x: x.split())\n",
    "\n",
    "    # Create a Gensim dictionary and corpus\n",
    "    dictionary = corpora.Dictionary(tokenized_text)\n",
    "    corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "\n",
    "    # Train an LDA model\n",
    "    lda_model = gensim.models.LdaModel(\n",
    "        corpus=corpus, id2word=dictionary, num_topics=n_topics, passes=10, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Extract LDA topics\n",
    "    topics = lda_model[corpus]\n",
    "\n",
    "    for topic in lda_model[corpus]:\n",
    "        print(topic)\n",
    "\n",
    "    print(f\"Number of topics extracted: {len(topics)}\")\n",
    "\n",
    "    # Add LDA topic distributions as new features\n",
    "    for i in range(n_topics):\n",
    "        df[f\"Topic_{i+1}\"] = [topic[i][1] if i < len(topic) else 0 for topic in topics]\n",
    "\n",
    "    return df, lda_model, corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_transform(data_frame: pd.DataFrame, t: float, columns: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transform specified columns in a DataFrame based on a threshold 't'.\n",
    "\n",
    "    Parameters\n",
    "    -------\n",
    "        data_frame : pd.DataFrame\n",
    "            The DataFrame to be transformed.\n",
    "        t : float\n",
    "            The threshold value.\n",
    "        columns_to_transform : list\n",
    "            List of column names to be transformed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        pandas.DataFrame: A new DataFrame with specified columns transformed.\n",
    "    \"\"\"\n",
    "    transformed_df = data_frame.copy()\n",
    "\n",
    "    for column in columns:\n",
    "        transformed_df[column] = (transformed_df[column] >= t).astype(int)\n",
    "\n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, meta = loadarff(DATA_PATH)\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimensions of the dataset:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove b string from data\n",
    "str_df = data.select_dtypes([object])\n",
    "str_df = str_df.stack().str.decode(\"utf-8\").unstack()\n",
    "data = pd.concat([str_df, data.select_dtypes(exclude=[object])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply preprocessing to data\n",
    "data = preprocessor(\n",
    "    DATA_PATH, [\"Model\", \"Charge\", \"Driver.City\", \"Arrest.Type\", \"Commercial.Vehicle\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stopwords and en_core_web_sm used in preprocess_text\n",
    "nltk.download(\"stopwords\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = data[\"description_clean\"].apply(lambda x: x.split())\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "\n",
    "topic_range = range(2, 21)\n",
    "\n",
    "models = []\n",
    "coherence_scores = []\n",
    "\n",
    "for n_topics in topic_range:\n",
    "    lda_model = gensim.models.LdaModel(\n",
    "        corpus=corpus, id2word=dictionary, num_topics=n_topics, passes=10\n",
    "    )\n",
    "    models.append(lda_model)\n",
    "    coherence_model = CoherenceModel(\n",
    "        model=lda_model,\n",
    "        texts=data[\"description_clean\"].str.split(),\n",
    "        dictionary=dictionary,\n",
    "        coherence=\"c_v\",\n",
    "    )\n",
    "    coherence_scores.append(coherence_model.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the coherence scores\n",
    "plt.plot(topic_range, coherence_scores)\n",
    "plt.xticks(np.arange(2, 21, step=1))\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "plt.title(\"Coherence Score vs. Number of Topics\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, lda_model, corpus, dictionary = create_n_topics(\n",
    "    data, column_name=\"description_clean\", n_topics=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(3):\n",
    "    plt.hist(data[f\"Topic_{i+1}\"], alpha=0.5, label=f\"Topic_{i+1}\", bins=20)\n",
    "\n",
    "plt.xlabel(\"Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Topics\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualing the topic seperation using pyLDAvis\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = threshold_transform(data, t=0.334, columns=[\"Topic_1\", \"Topic_2\", \"Topic_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = data.select_dtypes(exclude=[object])\n",
    "# shows correlation of numeric features\n",
    "corr = df_numeric.corr()\n",
    "corr\n",
    "\n",
    "# create heat map of correlation plot\n",
    "sns.heatmap(corr)\n",
    "plt.title(\"Correlation plot of numerical features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
