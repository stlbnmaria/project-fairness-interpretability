{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the two datasources - Traffic violations\n",
    "\n",
    "For the same topic, two different versions of the same dataset were identified, which were both published on openML. \n",
    "\n",
    "The first is unprocessed and can be found [here](https://api.openml.org/d/42132). The second one is a preprocessed and subsampled version that can be downloaded [here](https://www.openml.org/search?type=data&status=active&sort=runs&order=desc&id=42345). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import datetime\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyLDAvis.gensim\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.io.arff import loadarff\n",
    "from scipy.io.arff._arffread import MetaData\n",
    "\n",
    "from src.data_preprocessing.data_preprocessor import (\n",
    "    load_data,\n",
    "    change_to_numeric,\n",
    "    feature_engineering,\n",
    "    transform_label,\n",
    "    convert_float_to_int,\n",
    "    drop_cols,\n",
    "    filter_na,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_DATA_PATH = Path(\"../data/dataset.csv\")\n",
    "DATA_PATH = Path(\"../data/file65ef3a759daf.arff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, meta = loadarff(DATA_PATH)\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimensions of the dataset:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove b string from data\n",
    "str_df = data.select_dtypes([object])\n",
    "str_df = str_df.stack().str.decode(\"utf-8\").unstack()\n",
    "data = pd.concat([str_df, data.select_dtypes(exclude=[object])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data(path_: Path) -> Tuple[pd.DataFrame, MetaData]:\n",
    "#     \"\"\"Loads the .arff file (incl. metadata) and converts to utf-8.\n",
    "\n",
    "#     Parameters\n",
    "#     -------\n",
    "#     path_ : Path\n",
    "#             Path of the data.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     data : pd.DataFrame\n",
    "#             Data as a dataframe.\n",
    "#     meta : scipy.io.arff._arffread.Metadata\n",
    "#             Metadata of the dataset.\n",
    "#     \"\"\"\n",
    "#     # load df and metadata from .arff\n",
    "#     data, meta = loadarff(path_)\n",
    "#     data = pd.DataFrame(data)\n",
    "\n",
    "#     # remove b string from data\n",
    "#     str_df = data.select_dtypes([object])\n",
    "#     str_df = str_df.reset_index().melt(id_vars=\"index\").set_index(\"index\")\n",
    "#     str_df[\"value\"] = str_df[\"value\"].str.decode(\"utf-8\")\n",
    "\n",
    "#     # rename the 'value' column to avoid conflicts and perform pivot\n",
    "#     str_df = str_df.rename(columns={\"value\": \"decoded_value\"})\n",
    "#     str_df = pd.pivot_table(\n",
    "#         str_df, columns=\"variable\", values=\"decoded_value\", index=\"index\", aggfunc=lambda x: x\n",
    "#     )\n",
    "\n",
    "#     # reset both the column and index names to None\n",
    "#     str_df = str_df.rename_axis(index=None, columns=None)\n",
    "\n",
    "#     # merge str and non-str columns\n",
    "#     data = pd.concat([str_df, data.select_dtypes(exclude=[object])], axis=1)\n",
    "\n",
    "#     return data, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def change_to_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"Change yes/no values in columns to 0/1.\n",
    "\n",
    "#     Parameters\n",
    "#     -------\n",
    "#     df : pd.DataFrame\n",
    "#             Data to transform.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     df : pd.DataFrame\n",
    "#             Transformed data.\n",
    "#     \"\"\"\n",
    "#     for col in df.columns:\n",
    "#         # only change columns that have no missing values to 0 / 1\n",
    "#         if set(df[col].unique().tolist()) - set([\"No\", \"Yes\"]) == set():\n",
    "#             df[col] = df[col].map(dict(Yes=1, No=0))\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def feature_engineering(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"Creates new features based and drops the old ones.\n",
    "\n",
    "#         - If the Vehicle's State is Maryland (MD)\n",
    "#         - If the Driver's State is Maryland (MD)\n",
    "#         - If the Driver License's State is Maryland (MD)\n",
    "\n",
    "#     Parameters\n",
    "#     -------\n",
    "#     df : pd.DataFrame\n",
    "#             Data to transform.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     df : pd.DataFrame\n",
    "#             Transformed data.\n",
    "#     \"\"\"\n",
    "#     # Vehicle's State\n",
    "#     df[\"State_MD\"] = (df[\"State\"] == \"MD\").astype(int)\n",
    "#     df.loc[df[\"State\"] == \"?\", \"State_MD\"] = -1\n",
    "\n",
    "#     # Driver's State\n",
    "#     df[\"Driver_State_MD\"] = (df[\"Driver.State\"] == \"MD\").astype(int)\n",
    "#     df.loc[df[\"Driver.State\"] == \"?\", \"Driver_State_MD\"] = -1\n",
    "\n",
    "#     # Driver License's State\n",
    "#     df[\"DL_State_MD\"] = (df[\"DL.State\"] == \"MD\").astype(int)\n",
    "#     df.loc[df[\"DL.State\"] == \"?\", \"DL_State_MD\"] = -1\n",
    "\n",
    "#     df = df.drop(columns=[\"State\", \"Driver.State\", \"DL.State\"])\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform_label(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"Drops rows that are equal to SERO and changes label to \"Citation\" with 0/1 - values.\n",
    "\n",
    "#     Parameters\n",
    "#     -------\n",
    "#     df : pd.DataFrame\n",
    "#             Data to transform.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     df : pd.DataFrame\n",
    "#             Transformed data.\n",
    "#     \"\"\"\n",
    "#     df = df[df[\"Violation.Type\"] != \"SERO\"].copy()\n",
    "#     df[\"Citation\"] = df.loc[:, \"Violation.Type\"].apply(lambda x: 1 if x == \"Citation\" else 0)\n",
    "#     df = df.drop(columns=[\"Violation.Type\"])\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_float_to_int(df: pd.DataFrame, column_name: str = \"Year\") -> pd.DataFrame:\n",
    "#     \"\"\"If possible to convert float to int converts to int.\n",
    "\n",
    "#     Parameters\n",
    "#     -------\n",
    "#     df : pd.DataFrame\n",
    "#             Data to transform.\n",
    "#     column_name : str\n",
    "#             Column that should be converted to int.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     df : pd.DataFrame\n",
    "#             Transformed data.\n",
    "#     \"\"\"\n",
    "#     assert (\n",
    "#         df[column_name].dropna().apply(lambda x: x.is_integer()).all()\n",
    "#     ), \"Can't be converted to int\"\n",
    "#     df[column_name] = df[column_name].fillna(-1).astype(int)\n",
    "\n",
    "#     # get today's year and filter by -1 (na), above 1990 or below/equal today's year\n",
    "#     year = int(datetime.date.today().strftime(\"%Y\"))\n",
    "#     df = df[(df[column_name] == -1) | ((df[column_name] > 1990) & (df[column_name] <= year))]\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def drop_cols(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "#     \"\"\"Drop columns from dataframe.\n",
    "\n",
    "#     Parameters\n",
    "#     -------\n",
    "#     df : pd.DataFrame\n",
    "#             Data to transform.\n",
    "#     cols : List\n",
    "#             List of columns to drop.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     df : pd.DataFrame\n",
    "#             Transformed data.\n",
    "#     \"\"\"\n",
    "#     df = df.drop(columns=cols)\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_na(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"Filter na-values in full df.\n",
    "\n",
    "#     Since analysis showed that values are missing at random\n",
    "#     across groups, make less than 1% of instances and instance normally has\n",
    "#     multiple missing feature values, this operation is valid.\n",
    "\n",
    "#     Parameters\n",
    "#     -------\n",
    "#     df : pd.DataFrame\n",
    "#             Data to transform.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     df : pd.DataFrame\n",
    "#             Transformed data.\n",
    "#     \"\"\"\n",
    "#     # filter out -1 in integer columns\n",
    "#     cols = df.select_dtypes([int]).columns\n",
    "#     for col in cols:\n",
    "#         df = df[df[col] != -1].copy()\n",
    "\n",
    "#     # filter out ? in string columns\n",
    "#     cols = df.select_dtypes([object]).columns\n",
    "#     for col in cols:\n",
    "#         df = df[~df[col].isin([\"U\", \"?\"])].copy()\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(data_path: Path, cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Load data and perform preprocessing steps.\n",
    "\n",
    "    Parameters\n",
    "    -------\n",
    "    path_ : Path\n",
    "            Path of the data.\n",
    "    cols : List\n",
    "            List of columns to drop.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : pd.DataFrame\n",
    "            Processed data.\n",
    "    \"\"\"\n",
    "    # load the data\n",
    "    data, _ = load_data(data_path)\n",
    "\n",
    "    # convert yes/no to 0/1 and year to int\n",
    "    data = change_to_numeric(data)\n",
    "    data = convert_float_to_int(data)\n",
    "\n",
    "    # perform feature engineering on state columns\n",
    "    data = feature_engineering(data)\n",
    "\n",
    "    # drop unwished cols\n",
    "    data = drop_cols(data, cols)\n",
    "\n",
    "    # transform label to 0/1 for citation\n",
    "    data = transform_label(data)\n",
    "\n",
    "    # filter na\n",
    "    data = filter_na(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply preprocessing to data\n",
    "data = preprocessor(\n",
    "    DATA_PATH, [\"Model\", \"Charge\", \"Driver.City\", \"Arrest.Type\", \"Commercial.Vehicle\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stopwords and en_core_web_sm used in preprocess_text\n",
    "nltk.download(\"stopwords\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df: pd.DataFrame, column_name: str = \"Description\") -> pd.DataFrame:\n",
    "    \"\"\"reformats text so that LDA can be applied in the next step\n",
    "\n",
    "    Parameters\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "            Data to transform.\n",
    "    column_name : (str, optional)\n",
    "            Name of column to be transformed.\n",
    "            Defaults to \"Description\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "            Transformed data.\n",
    "    \"\"\"\n",
    "    # making text lower case\n",
    "    lowercase_text = df[column_name].apply(lambda x: x.lower())\n",
    "\n",
    "    # tokenize text\n",
    "    tokenized_text = lowercase_text.apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "    # removing stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    clean_text = tokenized_text.apply(\n",
    "        lambda tokens: [\n",
    "            word for word in tokens if word not in stop_words and word not in string.punctuation\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # lemmatize words\n",
    "    lemmatized_text = clean_text.apply(\n",
    "        lambda tokens: [token.lemma_ for token in nlp(\" \".join(tokens))]\n",
    "    )\n",
    "\n",
    "    df[\"description_clean\"] = lemmatized_text.apply(lambda lem_tokens: \" \".join(lem_tokens))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = data[\"description_clean\"].apply(lambda x: x.split())\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "\n",
    "topic_range = range(2, 21)\n",
    "\n",
    "models = []\n",
    "coherence_scores = []\n",
    "\n",
    "for n_topics in topic_range:\n",
    "    lda_model = gensim.models.LdaModel(\n",
    "        corpus=corpus, id2word=dictionary, num_topics=n_topics, passes=10\n",
    "    )\n",
    "    models.append(lda_model)\n",
    "    coherence_model = CoherenceModel(\n",
    "        model=lda_model,\n",
    "        texts=data[\"description_clean\"].str.split(),\n",
    "        dictionary=dictionary,\n",
    "        coherence=\"c_v\",\n",
    "    )\n",
    "    coherence_scores.append(coherence_model.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the coherence scores\n",
    "plt.plot(topic_range, coherence_scores)\n",
    "plt.xticks(np.arange(2, 21, step=1))\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "plt.title(\"Coherence Score vs. Number of Topics\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_topics(\n",
    "    df: pd.DataFrame,\n",
    "    column_name: str = \"description_clean\",\n",
    "    n_topics: int = 10,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Applies LDA to a text column of DF and adds LDA topic distributions as new features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Data to transform.\n",
    "\n",
    "    column_name : str, optional\n",
    "        Name of the column to be transformed. Defaults to \"description_clean\".\n",
    "\n",
    "    num_topics : int, optional\n",
    "        Number of topics for LDA. Defaults to 10.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        Transformed data with added LDA topic features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Preprocess text\n",
    "    df = preprocess_text(df, column_name)\n",
    "\n",
    "    # Tokenized text is already available from preprocessing but needs to be list of list\n",
    "    tokenized_text = df[column_name].apply(lambda x: x.split())\n",
    "\n",
    "    # Create a Gensim dictionary and corpus\n",
    "    dictionary = corpora.Dictionary(tokenized_text)\n",
    "    corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "\n",
    "    # Train an LDA model\n",
    "    lda_model = gensim.models.LdaModel(\n",
    "        corpus=corpus, id2word=dictionary, num_topics=n_topics, passes=10\n",
    "    )\n",
    "\n",
    "    # Extract LDA topics\n",
    "    topics = lda_model[corpus]\n",
    "\n",
    "    for topic in lda_model[corpus]:\n",
    "        print(topic)\n",
    "\n",
    "    print(f\"Number of topics extracted: {len(topics)}\")\n",
    "\n",
    "    # Add LDA topic distributions as new features\n",
    "    for i in range(n_topics):\n",
    "        df[f\"Topic_{i+1}\"] = [topic[i][1] if i < len(topic) else 0 for topic in topics]\n",
    "\n",
    "    return df, lda_model, corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, lda_model, corpus, dictionary = create_n_topics(\n",
    "    data, column_name=\"description_clean\", n_topics=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(3):\n",
    "    plt.hist(data[f\"Topic_{i+1}\"], alpha=0.5, label=f\"Topic_{i+1}\", bins=20)\n",
    "\n",
    "plt.xlabel(\"Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Topics\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualing the topic seperation using pyLDAvis\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_transform(data_frame: pd.DataFrame, t: float, columns: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transform specified columns in a DataFrame based on a threshold 't'.\n",
    "\n",
    "    Parameters\n",
    "    -------\n",
    "        data_frame : pd.DataFrame\n",
    "            The DataFrame to be transformed.\n",
    "        t : float\n",
    "            The threshold value.\n",
    "        columns_to_transform : list\n",
    "            List of column names to be transformed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        pandas.DataFrame: A new DataFrame with specified columns transformed.\n",
    "    \"\"\"\n",
    "    transformed_df = data_frame.copy()\n",
    "\n",
    "    for column in columns:\n",
    "        transformed_df[column] = (transformed_df[column] >= t).astype(int)\n",
    "\n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = threshold_transform(data, t=0.334, columns=[\"Topic_1\", \"Topic_2\", \"Topic_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = data.select_dtypes(exclude=[object])\n",
    "# shows correlation of numeric features\n",
    "corr = df_numeric.corr()\n",
    "corr\n",
    "\n",
    "# create heat map of correlation plot\n",
    "sns.heatmap(corr)\n",
    "plt.title(\"Correlation plot of numerical features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
