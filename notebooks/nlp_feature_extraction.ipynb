{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the two datasources - Traffic violations\n",
    "\n",
    "For the same topic, two different versions of the same dataset were identified, which were both published on openML. \n",
    "\n",
    "The first is unprocessed and can be found [here](https://api.openml.org/d/42132). The second one is a preprocessed and subsampled version that can be downloaded [here](https://www.openml.org/search?type=data&status=active&sort=runs&order=desc&id=42345). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from scipy.io.arff import loadarff\n",
    "from scipy.io.arff._arffread import MetaData\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_DATA_PATH = Path(\"../data/dataset.csv\")\n",
    "DATA_PATH = Path(\"../data/file65ef3a759daf.arff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, meta = loadarff(DATA_PATH)\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimensions of the dataset:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove b string from data\n",
    "str_df = data.select_dtypes([object])\n",
    "str_df = str_df.stack().str.decode(\"utf-8\").unstack()\n",
    "data = pd.concat([str_df, data.select_dtypes(exclude=[object])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path_: Path) -> Tuple[pd.DataFrame, MetaData]:\n",
    "    \"\"\"Loads the .arff file (incl. metadata) and converts to utf-8.\n",
    "\n",
    "    Parameters\n",
    "    -------\n",
    "    path_ : Path\n",
    "            Path of the data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : pd.DataFrame\n",
    "            Data as a dataframe.\n",
    "    meta : scipy.io.arff._arffread.Metadata\n",
    "            Metadata of the dataset.\n",
    "    \"\"\"\n",
    "    # load df and metadata from .arff\n",
    "    data, meta = loadarff(path_)\n",
    "    data = pd.DataFrame(data)\n",
    "\n",
    "    # remove b string from data\n",
    "    str_df = data.select_dtypes([object])\n",
    "    str_df = str_df.reset_index().melt(id_vars=\"index\").set_index(\"index\")\n",
    "    str_df[\"value\"] = str_df[\"value\"].str.decode(\"utf-8\")\n",
    "\n",
    "    # rename the 'value' column to avoid conflicts and perform pivot\n",
    "    str_df = str_df.rename(columns={\"value\": \"decoded_value\"})\n",
    "    str_df = pd.pivot_table(\n",
    "        str_df, columns=\"variable\", values=\"decoded_value\", index=\"index\", aggfunc=lambda x: x\n",
    "    )\n",
    "\n",
    "    # reset both the column and index names to None\n",
    "    str_df = str_df.rename_axis(index=None, columns=None)\n",
    "\n",
    "    # merge str and non-str columns\n",
    "    data = pd.concat([str_df, data.select_dtypes(exclude=[object])], axis=1)\n",
    "\n",
    "    return data, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_to_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Change yes/no values in columns to 0/1.\n",
    "\n",
    "    Parameters\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "            Data to transform.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "            Transformed data.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        # only change columns that have no missing values to 0 / 1\n",
    "        if set(df[col].unique().tolist()) - set([\"No\", \"Yes\"]) == set():\n",
    "            df[col] = df[col].map(dict(Yes=1, No=0))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Creates new features based and drops the old ones.\n",
    "\n",
    "        - If the Vehicle's State is Maryland (MD)\n",
    "        - If the Driver's State is Maryland (MD)\n",
    "        - If the Driver License's State is Maryland (MD)\n",
    "\n",
    "    Parameters\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "            Data to transform.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "            Transformed data.\n",
    "    \"\"\"\n",
    "    # Vehicle's State\n",
    "    df[\"State_MD\"] = (df[\"State\"] == \"MD\").astype(int)\n",
    "    df.loc[df[\"State\"] == \"?\", \"State_MD\"] = -1\n",
    "\n",
    "    # Driver's State\n",
    "    df[\"Driver_State_MD\"] = (df[\"Driver.State\"] == \"MD\").astype(int)\n",
    "    df.loc[df[\"Driver.State\"] == \"?\", \"Driver_State_MD\"] = -1\n",
    "\n",
    "    # Driver License's State\n",
    "    df[\"DL_State_MD\"] = (df[\"DL.State\"] == \"MD\").astype(int)\n",
    "    df.loc[df[\"DL.State\"] == \"?\", \"DL_State_MD\"] = -1\n",
    "\n",
    "    df = df.drop(columns=[\"State\", \"Driver.State\", \"DL.State\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_label(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drops rows that are equal to SERO and changes label to \"Citation\" with 0/1 - values.\n",
    "\n",
    "    Parameters\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "            Data to transform.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "            Transformed data.\n",
    "    \"\"\"\n",
    "    df = df[df[\"Violation.Type\"] != \"SERO\"].copy()\n",
    "    df[\"Citation\"] = df.loc[:, \"Violation.Type\"].apply(lambda x: 1 if x == \"Citation\" else 0)\n",
    "    df = df.drop(columns=[\"Violation.Type\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_float_to_int(df: pd.DataFrame, column_name: str = \"Year\") -> pd.DataFrame:\n",
    "    \"\"\"If possible to convert float to int converts to int.\n",
    "\n",
    "    Parameters\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "            Data to transform.\n",
    "    column_name : str\n",
    "            Column that should be converted to int.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "            Transformed data.\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        df[column_name].dropna().apply(lambda x: x.is_integer()).all()\n",
    "    ), \"Can't be converted to int\"\n",
    "    df[column_name] = df[column_name].fillna(-1).astype(int)\n",
    "\n",
    "    # get today's year and filter by -1 (na), above 1990 or below/equal today's year\n",
    "    year = int(datetime.date.today().strftime(\"%Y\"))\n",
    "    df = df[(df[column_name] == -1) | ((df[column_name] > 1990) & (df[column_name] <= year))]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cols(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Drop columns from dataframe.\n",
    "\n",
    "    Parameters\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "            Data to transform.\n",
    "    cols : List\n",
    "            List of columns to drop.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "            Transformed data.\n",
    "    \"\"\"\n",
    "    df = df.drop(columns=cols)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_na(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Filter na-values in full df.\n",
    "\n",
    "    Since analysis showed that values are missing at random\n",
    "    across groups, make less than 1% of instances and instance normally has\n",
    "    multiple missing feature values, this operation is valid.\n",
    "\n",
    "    Parameters\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "            Data to transform.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "            Transformed data.\n",
    "    \"\"\"\n",
    "    # filter out -1 in integer columns\n",
    "    cols = df.select_dtypes([int]).columns\n",
    "    for col in cols:\n",
    "        df = df[df[col] != -1].copy()\n",
    "\n",
    "    # filter out ? in string columns\n",
    "    cols = df.select_dtypes([object]).columns\n",
    "    for col in cols:\n",
    "        df = df[~df[col].isin([\"U\", \"?\"])].copy()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(data_path: Path, cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Load data and perform preprocessing steps.\n",
    "\n",
    "    Parameters\n",
    "    -------\n",
    "    path_ : Path\n",
    "            Path of the data.\n",
    "    cols : List\n",
    "            List of columns to drop.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : pd.DataFrame\n",
    "            Processed data.\n",
    "    \"\"\"\n",
    "    # load the data\n",
    "    data, _ = load_data(data_path)\n",
    "\n",
    "    # convert yes/no to 0/1 and year to int\n",
    "    data = change_to_numeric(data)\n",
    "    data = convert_float_to_int(data)\n",
    "\n",
    "    # perform feature engineering on state columns\n",
    "    data = feature_engineering(data)\n",
    "\n",
    "    # drop unwished cols\n",
    "    data = drop_cols(data, cols)\n",
    "\n",
    "    # transform label to 0/1 for citation\n",
    "    data = transform_label(data)\n",
    "\n",
    "    # filter na\n",
    "    data = filter_na(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocessor(\n",
    "    DATA_PATH, [\"Model\", \"Charge\", \"Driver.City\", \"Arrest.Type\", \"Commercial.Vehicle\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df: pd.DataFrame, column_name: str = \"Description\") -> pd.DataFrame:\n",
    "    \"\"\"reformats text so that LDA can be applied in the next step\n",
    "\n",
    "    Parameters\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "            Data to transform.\n",
    "    column_name : (str, optional)\n",
    "            Name of column to be transformed.\n",
    "            Defaults to \"Description\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "            Transformed data.\n",
    "    \"\"\"\n",
    "    # making text lower case\n",
    "    lowercase_text = df[column_name].apply(lambda x: x.lower())\n",
    "\n",
    "    # tokenize text\n",
    "    tokenized_text = lowercase_text.apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "    # removing stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    clean_text = tokenized_text.apply(\n",
    "        lambda tokens: [\n",
    "            word for word in tokens if word not in stop_words and word not in string.punctuation\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # lemmatize words\n",
    "    lemmatized_text = clean_text.apply(\n",
    "        lambda tokens: [token.lemma_ for token in nlp(\" \".join(tokens))]\n",
    "    )\n",
    "\n",
    "    df[\"description_clean\"] = lemmatized_text.apply(lambda lem_tokens: \" \".join(lem_tokens))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['description_clean'] = data['Description'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a CountVectorizer\n",
    "# vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "# X = vectorizer.fit_transform(data['description_clean'])\n",
    "\n",
    "# # create an LDA model\n",
    "# num_topics = 20\n",
    "# lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "\n",
    "# lda.fit(X)\n",
    "\n",
    "# topic_distributions = lda.transform(X)\n",
    "\n",
    "# for i in range(num_topics):\n",
    "#     data[f\"Topic_{i+1}\"] = topic_distributions[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_topics(\n",
    "    df: pd.DataFrame,\n",
    "    column_name: str = \"description_clean\",\n",
    "    n_topics: int = 10,\n",
    "    max_features: int = 1000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies Latent Dirichlet Allocation (LDA) to a text column in a DataFrame and adds LDA topic distributions as new features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Data to transform.\n",
    "\n",
    "    column_name : str, optional\n",
    "        Name of the column to be transformed. Defaults to \"Description\".\n",
    "\n",
    "    num_topics : int, optional\n",
    "        Number of topics for LDA. Defaults to 10.\n",
    "\n",
    "    max_features : int, optional\n",
    "        Maximum number of features for CountVectorizer. Defaults to 1000.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        Transformed data with added LDA topic features.\n",
    "    \"\"\"\n",
    "    # Create a CountVectorizer\n",
    "    vectorizer = CountVectorizer(max_features=max_features, stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform(data[column_name])\n",
    "\n",
    "    # create an LDA model\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "\n",
    "    lda.fit(X)\n",
    "\n",
    "    topic_distributions = lda.transform(X)\n",
    "\n",
    "    for i in range(n_topics):\n",
    "        df[f\"Topic_{i+1}\"] = topic_distributions[:, i]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_n_topics(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = data.select_dtypes(exclude=[object])\n",
    "# shows correlation of numeric features\n",
    "corr = df_numeric.corr()\n",
    "corr\n",
    "\n",
    "sns.heatmap(corr)\n",
    "plt.title(\"Correlation plot of numerical features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_encoder(\n",
    "    df: pd.DataFrame = None,\n",
    "    cols: list = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Encode the categorical variables within the dataframe.\n",
    "\n",
    "    Parameters\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "            Dataframe to split.\n",
    "    cols : list\n",
    "            List with the categorical variables we wish to dummy encode.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        Dataframe with categorical variables one-hot-encoded(original df if catboost=True).\n",
    "    \"\"\"\n",
    "    df = pd.get_dummies(df, cols, dtype=int)\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
