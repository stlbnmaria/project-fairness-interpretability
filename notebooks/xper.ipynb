{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import XPER\n",
    "\n",
    "from pathlib import Path\n",
    "from src.utils.models_pkl import load_pickle\n",
    "from src.modeling.create_data_split import split_data\n",
    "from config.config_modeling import CAT_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH = Path(\"../data/data.csv\")\n",
    "df = pd.read_csv(OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(\"../models/XGB.pkl\")\n",
    "model = load_pickle(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = split_data(cols=CAT_COLS, df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = data[\"train\"][0], data[\"train\"][1], data[\"val\"][0], data[\"val\"][1]\n",
    "X_test, y_test = data[\"test\"][0], data[\"test\"][1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XPER Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from XPER.compute.Performance import ModelPerformance\n",
    "\n",
    "# Define the evaluation metric(s) to be used\n",
    "XPER = ModelPerformance(X_train, y_train, X_test, y_test, model)\n",
    "\n",
    "# Evaluate the model performance using the specified metric(s)\n",
    "PM = XPER.evaluate([\"AUC\"])\n",
    "\n",
    "# Print the performance metrics\n",
    "print(\"Performance Metrics: \", round(PM, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from XPER.compute.Performance import ModelPerformance\n",
    "\n",
    "# XPER for train\n",
    "# Define the evaluation metric(s) to be used\n",
    "XPER = ModelPerformance(X_train, y_train, X_train, y_train, model)\n",
    "\n",
    "# Evaluate the model performance using the specified metric(s)\n",
    "PM = XPER.evaluate([\"AUC\"])\n",
    "\n",
    "# Print the performance metrics\n",
    "print(\"Performance Metrics: \", round(PM, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate XPER values for the model's performance\n",
    "XPER_values = XPER.calculate_XPER_values([\"AUC\"])\n",
    "# AUC takes much longer than Precision! For illustration purpose, it is better to choose precision."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Calculate permutation importance using eli5\n",
    "perm_importance = eli5.sklearn.PermutationImportance(\n",
    "    model, scoring=\"roc_auc\", random_state=42, n_iter=30\n",
    ")\n",
    "perm_importance.fit(X_test, y_test)\n",
    "\n",
    "# Display feature importances\n",
    "eli5.show_weights(perm_importance, feature_names=list(X_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "importances = perm_importance.feature_importances_\n",
    "percentage_contributions = (importances / importances.sum()) * 100\n",
    "\n",
    "# Display feature importances and create a bar plot\n",
    "feature_names = list(X_train.columns)  # Replace with your feature names\n",
    "\n",
    "# Sort feature importances in descending order for plotting\n",
    "sorted_indices = np.argsort(percentage_contributions)[::-1]\n",
    "sorted_features = [feature_names[i] for i in sorted_indices]\n",
    "sorted_contributions = [percentage_contributions[i] for i in sorted_indices]\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(sorted_features)), sorted_contributions, tick_label=sorted_features)\n",
    "plt.title(\"Percentage Contribution to AUC (Permutation Importance)\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Percentage Contribution (%)\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "32fd20a7cb783056e90091779fc1ae5d4f04b827b5afc79ecc17343324d2dcd5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
